
import os
from airflow import DAG
from airflow.operators.python_operator import PythonOperator
from datetime import datetime, timedelta
import pandas as pd
import time
import fastf1

default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'start_date': datetime(2024, 3, 3),
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 0,
    'retry_delay': timedelta(minutes=5),
}

dag = DAG(
    'fetch_f1_session_data_new',
    default_args=default_args,
    description='A DAG to fetch F1 session data, extract driver details and store it in CSV',
    catchup=False,
)

def fetch_session_data(year, location, identifier, csv_file_path):
    all_sessions = pd.DataFrame(columns=['Year', 'Location', 'Identifier', 'DriverNumber', 'BroadcastName', 'Abbreviation', 'DriverId', 'TeamName',
                                            'TeamColor', 'TeamId', 'FirstName', 'LastName', 'FullName',
                                            'HeadshotUrl', 'CountryCode', 'Position', 'ClassifiedPosition',
                                            'GridPosition', 'Q1', 'Q2', 'Q3', 'Time', 'Status', 'Points'])

    try:
        session_data = fastf1.get_session(year, location, identifier)
        session_data.load()

        session_data_df = session_data.results
        session_data_df['Year'] = year
        session_data_df['Location'] = location
        session_data_df['Identifier'] = identifier

        if not session_data_df.empty:
            if not os.path.exists(csv_file_path) or os.path.getsize(csv_file_path) == 0:
                all_sessions.to_csv(csv_file_path, mode='a', index=False, header=True)
                print(f"Column headers: {all_sessions} written to: {csv_file_path}")

            all_sessions = pd.concat([all_sessions, session_data_df], ignore_index=True)
            all_sessions.to_csv(csv_file_path, mode='a', index=False, header=False)
            print(f"Session data appended to: {csv_file_path}")

    except ValueError as e:
        print(f"Error: {e}")
    except KeyError as e:
        print(f"KeyError: 'DriverNumber' key not found in driver_info: {e}")
    except AttributeError as e:
        print(f"AttributeError: 'driver_info' attribute not found in session_data: {e}")

def fetch_all_sessions():
    identifiers = ['R', 'Q', 'S', 'SQ', 'SS', 'FP1', 'FP2', 'FP3']
    csv_file_path = "/opt/airflow/dags/session_data_1950_to_2023.csv"
    api_calls_in_hour = 0
    start_time = time.time()
    
    # Check if CSV file exists and get the last date recorded
    if os.path.isfile(csv_file_path):
        last_date_df = pd.read_csv(csv_file_path, usecols=['Year', 'Location', 'Identifier'])
        last_year, last_location, last_identifier = last_date_df.iloc[-1]
        last_year = int(last_year)
    else:
        last_year, last_location, last_identifier = None, None, None
    
    for year in range(last_year or 1950, 2023):
        print(f"YEAR>>> {year}")
        event_schedule = fastf1.get_event_schedule(year)
        unique_locations = event_schedule['Location'].unique()
        print(f"LOCATION>>> {unique_locations}")

        for location in unique_locations:
            print(f"location>>> {location}")
            
            # Start from the last recorded date in the CSV file
            if year == last_year and location == last_location:
                idx = identifiers.index(last_identifier)
                identifiers = identifiers[idx:]
                
            for identifier in identifiers:
                fetch_session_data(year, location, identifier, csv_file_path)
                api_calls_in_hour += 1
                
                if api_calls_in_hour == 190:
                    print("API LIMIT REACHED 190")
                    elapsed_time = time.time() - start_time
                    if elapsed_time < 3600:
                        time.sleep(3600 - elapsed_time)
                    start_time = time.time()
                    api_calls_in_hour = 0

fetch_all_sessions_task = PythonOperator(
    task_id='fetch_all_sessions',
    python_callable=fetch_all_sessions,
    dag=dag,
)

fetch_all_sessions_task
