from airflow import DAG
from airflow.operators.python_operator import PythonOperator
from datetime import datetime, timedelta
import pandas as pd
import time
import fastf1

default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'start_date': datetime(2024, 3, 3),
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 0,
    'retry_delay': timedelta(minutes=5),
}

dag = DAG(
    'fetch_f1_session_data',
    default_args=default_args,
    description='A DAG to fetch F1 session data, extract driver details and store it in CSV',
    catchup=False,
)

def fetch_session_data(year, location, identifier, csv_file_path):
    all_sessions = pd.DataFrame(columns=['Year', 'Location', 'Identifier'] +
                                           ['DriverNumber', 'BroadcastName', 'Abbreviation', 'DriverId', 'TeamName',
                                            'TeamColor', 'TeamId', 'FirstName', 'LastName', 'FullName',
                                            'HeadshotUrl', 'CountryCode', 'Position', 'ClassifiedPosition',
                                            'GridPosition', 'Q1', 'Q2', 'Q3', 'Time', 'Status', 'Points'])

    try:
        session_data = fastf1.get_session(year, location, identifier)
        session_data.load()

        session_data_df = session_data.results
        session_data_df['Year'] = year
        session_data_df['Location'] = location
        session_data_df['Identifier'] = identifier

        if not session_data_df.empty:
            all_sessions = pd.concat([all_sessions, session_data_df], ignore_index=True)
            all_sessions.to_csv(csv_file_path, mode='a', index=False, header=False)
            print(f"Session data appended to: {csv_file_path}")

    except ValueError as e:
        print(f"Error: {e}")
    except KeyError as e:
        print(f"KeyError: 'DriverNumber' key not found in driver_info: {e}")
    except AttributeError as e:
        print(f"AttributeError: 'driver_info' attribute not found in session_data: {e}")

def fetch_all_sessions():
    identifiers = ['R', 'Q', 'S', 'SQ', 'SS', 'FP1', 'FP2', 'FP3']
    csv_file_path = "/opt/airflow/dags/session_data_1950_to_2023.csv"
    api_calls_in_hour = 0
    start_time = time.time()
    
    for year in range(1950, 2023):
        print(f"YEAR>>> {year}")
        event_schedule = fastf1.get_event_schedule(year)
        unique_locations = event_schedule['Location'].unique()
        print(f"LOCATION>>> {unique_locations}")

        for location in unique_locations:
            print(f"location>>> {location}")
            for identifier in identifiers:
                fetch_session_data(year, location, identifier, csv_file_path)
                api_calls_in_hour += 1
                
                if api_calls_in_hour == 190:
                    print("API LIMIT REACHED 190")
                    elapsed_time = time.time() - start_time
                    if elapsed_time < 3600:
                        time.sleep(3600 - elapsed_time)
                    start_time = time.time()
                    api_calls_in_hour = 0

fetch_all_sessions_task = PythonOperator(
    task_id='fetch_all_sessions',
    python_callable=fetch_all_sessions,
    dag=dag,
)

fetch_all_sessions_task
